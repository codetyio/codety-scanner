[
  {
    "sha": "687a61fd8b99ba82a77f4017a9c8b3541c4ff462",
    "filename": "complete/src/main/java/com/example/springboot/Application.java",
    "status": "modified",
    "additions": 6,
    "deletions": 2,
    "changes": 8,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Fmain%2Fjava%2Fcom%2Fexample%2Fspringboot%2FApplication.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Fmain%2Fjava%2Fcom%2Fexample%2Fspringboot%2FApplication.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Fmain%2Fjava%2Fcom%2Fexample%2Fspringboot%2FApplication.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -10,22 +10,26 @@\n \n @SpringBootApplication\n public class Application {\n-\n+//fwejfow\n \tpublic static void main(String[] args) {\n \t\tSpringApplication.run(Application.class, args);\n \t}\n \t@Bean\n \tpublic CommandLineRunner commandLineRunner(ApplicationContext ctx) {\n \t\treturn args -> {\n \t\t\tSystem.out.println(\"Let's inspect the beans provided by Spring Boot:\");\n+//aaaaaa1\n+//aaa2\n \n \t\t\tString[] beanNames = ctx.getBeanDefinitionNames();\n \t\t\tArrays.sort(beanNames);\n \t\t\tfor (String beanName : beanNames) {\n \t\t\t\tSystem.out.println(beanName);\n \t\t\t}\n \n+\t\t\t//bbb\n+\t\t\t//bbb2\n \t\t};\n \t}\n-\n+//ccc\n }"
  },
  {
    "sha": "653680db813bdb604b61d6490c546334bca98ecd",
    "filename": "complete/src/test/java/com/example/springboot/HelloControllerIT.java",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FHelloControllerIT.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FHelloControllerIT.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FHelloControllerIT.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -15,6 +15,8 @@ public class HelloControllerIT {\n \t@Autowired\n \tprivate TestRestTemplate template;\n \n+    int a2 = 1;\n+\n     @Test\n     public void getHello() throws Exception {\n         ResponseEntity<String> response = template.getForEntity(\"/\", String.class);"
  },
  {
    "sha": "5e4c2c811ad781573ed073a46143befa3ae454fb",
    "filename": "complete/src/test/java/com/example/springboot/KafkaAdmin.java",
    "status": "added",
    "additions": 63,
    "deletions": 0,
    "changes": 63,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaAdmin.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaAdmin.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaAdmin.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,63 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.admin.*;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+\n+public class KafkaAdmin {\n+    static String topic = \"my\";\n+\n+    public static void main(String[] args) throws InterruptedException, ExecutionException {\n+//        test_producer();\n+//\n+        test_consumer();\n+\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException, ExecutionException {\n+\n+        Properties props = new Properties();\n+        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n+        props.put(\"max.buffer.bytes\", 1 << 20);\n+        props.put(\"connections.max.idle.ms\", 10000);\n+        props.put(\"request.timeout.ms\", 10000);\n+        AdminClient admin = AdminClient.create(props);\n+\n+        admin.close(Duration.ofSeconds(30));\n+\n+        Collection<ConsumerGroupListing> consumerGroupListings = admin.listConsumerGroups().valid().get();\n+        consumerGroupListings.forEach(System.out::println);\n+        ListTopicsResult topics = admin.listTopics();\n+        Set<String> names = topics.names().get();\n+        System.out.println(names);\n+\n+//\n+//        HashSet<String> topicNames = new HashSet<>();\n+//        String topicName = \"my\";\n+//        topicNames.add(topicName);\n+//        DescribeTopicsResult demoTopic = admin.describeTopics(topicNames);\n+//        try {\n+//            TopicDescription topicDescription = demoTopic.values().get(topicName).get();\n+//            System.out.println(\"Description of demo topic:\" + topicDescription);\n+//            if (topicDescription.partitions().size() != 2) {\n+//                System.out.println(\"Topic has wrong number of partitions. Exiting.\");\n+//                System.exit(-1);\n+//            }\n+//        } catch (ExecutionException e) {\n+//            // exit early for almost all exceptions\n+//            if (!(e.getCause() instanceof UnknownTopicOrPartitionException)) {\n+//                e.printStackTrace();\n+//                throw e;\n+//            }\n+//\n+//        }\n+//\n+//\n+//        }\n+    }\n+}"
  },
  {
    "sha": "5260339072b2310568addad4300ac8704ae28310",
    "filename": "complete/src/test/java/com/example/springboot/KafkaAvroClient.java",
    "status": "added",
    "additions": 36,
    "deletions": 0,
    "changes": 36,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaAvroClient.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaAvroClient.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaAvroClient.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,36 @@\n+package com.example.springboot;//package com.kafka;\n+//\n+//import com.avro.schema.User;\n+//import org.apache.kafka.clients.producer.KafkaProducer;\n+//import org.apache.kafka.clients.producer.Producer;\n+//import org.apache.kafka.clients.producer.ProducerRecord;\n+//\n+//import java.util.Properties;\n+//\n+//public class KafkaAvroClient {\n+//\n+//    public static void main(String[] args) {\n+//\n+//        Properties props = new Properties();\n+//        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+//        props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+//        props.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+//        props.put(\"schema.registry.url\", \"http://localhost:8000\");\n+//        String topic = \"my\";\n+//        Producer<String, User> producer = new KafkaProducer<>(props);\n+//        try {\n+//            User user = new User();\n+//            user.setFavoriteColor(\"red\");\n+//            user.setFavoriteNumber(12);\n+//            user.setName(\"Apple\");\n+//\n+//            ProducerRecord record = new ProducerRecord(topic, User.newBuilder(user).build());\n+//            producer.send(record).get();\n+//        } catch (Exception e) {\n+//            e.printStackTrace();\n+//        }\n+//        System.out.println(\"Sent out. \");\n+//\n+//    }\n+//\n+//}"
  },
  {
    "sha": "408a4f9b427d5f7f547fdfa128e042e4d6bb0bd2",
    "filename": "complete/src/test/java/com/example/springboot/KafkaMyWordCountConsumer.java",
    "status": "added",
    "additions": 81,
    "deletions": 0,
    "changes": 81,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,81 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+public class KafkaMyWordCountConsumer {\n+//    static String topic = \"my-word-by-year\";\n+    static String topic = \"test2\";\n+\n+    public static void main(String[] args) throws InterruptedException {\n+//        test_producer();\n+//\n+        test_consumer();\n+//\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException {\n+\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"group.id\", \"MyGroup1\");\n+//        props.put(\"group.id\", \"MyGroup\" + System.currentTimeMillis());\n+        props.put(\"key.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(\"value.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+//        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false \");\n+        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n+\n+        consumer.subscribe(Collections.singletonList(topic));\n+\n+        Duration timeout = Duration.ofMillis(100);\n+        System.out.println(\"Consuming.......\");\n+        while (true) {\n+            ConsumerRecords<String, String> records = consumer.poll(timeout);\n+            for (ConsumerRecord<String, String> record : records) {\n+                System.out.printf(System.currentTimeMillis() + \" KafkaMyWordCountConsumer::::: topic = %s, partition = %d, offset = %d, \" +\n+                                \"k = %s, value = %s\\n\",\n+                        record.topic(), record.partition(), record.offset(),\n+                        record.key(), record.value());\n+                //int updatedCount = 1;\n+                //System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+                consumer.commitAsync();\n+            }\n+            Thread.sleep(100);\n+        }\n+\n+\n+\n+    }\n+\n+    private static void test_producer() {\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"key.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"value.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"enable.idempotence\",  \"true\");\n+        props.put(\"acks\",  \"all\");\n+\n+\n+        ProducerRecord<String, String> record = new ProducerRecord<>(topic, \"a\", \"IdempotentMsg1\");\n+        KafkaProducer producer = new KafkaProducer<String, String>(props);\n+\n+        try {\n+            producer.send(record).get();\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+        }\n+        System.out.println(\"Sent out. \");\n+    }\n+}"
  },
  {
    "sha": "496574fe7b65840e80adf940d251db50637d901d",
    "filename": "complete/src/test/java/com/example/springboot/KafkaMyWordCountConsumer222.java",
    "status": "added",
    "additions": 80,
    "deletions": 0,
    "changes": 80,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer222.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer222.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer222.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,80 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+public class KafkaMyWordCountConsumer222 {\n+//    static String topic = \"my-word-by-year\";\n+    static String topic = \"test2\";\n+\n+    public static void main(String[] args) throws InterruptedException {\n+//        test_producer();\n+//\n+        test_consumer();\n+//\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException {\n+\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"group.id\", \"MyGroup1\");\n+//        props.put(\"group.id\", \"MyGroup\" + System.currentTimeMillis());\n+        props.put(\"key.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(\"value.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false \");\n+        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n+\n+        consumer.subscribe(Collections.singletonList(topic));\n+\n+        Duration timeout = Duration.ofMillis(100);\n+        System.out.println(\"Consuming.......\");\n+        while (true) {\n+            ConsumerRecords<String, String> records = consumer.poll(timeout);\n+            for (ConsumerRecord<String, String> record : records) {\n+                System.out.printf(System.currentTimeMillis() + \" KafkaMyWordCountConsumer::::: topic = %s, partition = %d, offset = %d, \" +\n+                                \"k = %s, value = %s\\n\",\n+                        record.topic(), record.partition(), record.offset(),\n+                        record.key(), record.value());\n+                //int updatedCount = 1;\n+                //System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+            }\n+            Thread.sleep(100);\n+        }\n+\n+\n+\n+    }\n+\n+    private static void test_producer() {\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"key.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"value.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"enable.idempotence\",  \"true\");\n+        props.put(\"acks\",  \"all\");\n+\n+\n+        ProducerRecord<String, String> record = new ProducerRecord<>(topic, \"a\", \"IdempotentMsg1\");\n+        KafkaProducer producer = new KafkaProducer<String, String>(props);\n+\n+        try {\n+            producer.send(record).get();\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+        }\n+        System.out.println(\"Sent out. \");\n+    }\n+}"
  },
  {
    "sha": "7f69c4f434d3a7b03146a6c37b77cca7e95e14da",
    "filename": "complete/src/test/java/com/example/springboot/KafkaMyWordCountConsumer4.java",
    "status": "added",
    "additions": 57,
    "deletions": 0,
    "changes": 57,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer4.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer4.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer4.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,57 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+public class KafkaMyWordCountConsumer4 {\n+    static String topic = \"my-word4-by-year\";\n+\n+    public static void main(String[] args) throws InterruptedException {\n+//        test_producer();\n+//\n+        test_consumer();\n+//\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException {\n+\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"group.id\", \"MyGroup\" + System.currentTimeMillis());\n+        props.put(\"key.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(\"value.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false \");\n+        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n+\n+        consumer.subscribe(Collections.singletonList(topic));\n+\n+        Duration timeout = Duration.ofMillis(100);\n+        System.out.println(\"Consuming.......\");\n+        while (true) {\n+            ConsumerRecords<String, String> records = consumer.poll(timeout);\n+            for (ConsumerRecord<String, String> record : records) {\n+                System.out.printf(System.currentTimeMillis() + \" KafkaMyWordCountConsumer::::: topic = %s, partition = %d, offset = %d, \" +\n+                                \"k = %s, value = %s\\n\",\n+                        record.topic(), record.partition(), record.offset(),\n+                        record.key(), record.value());\n+                //int updatedCount = 1;\n+                //System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+            }\n+            Thread.sleep(100);\n+        }\n+\n+\n+\n+    }\n+\n+}"
  },
  {
    "sha": "aa89b6ebfffd679a2d404ac200406e967d48a4c8",
    "filename": "complete/src/test/java/com/example/springboot/KafkaMyWordCountConsumer5.java",
    "status": "added",
    "additions": 57,
    "deletions": 0,
    "changes": 57,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer5.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer5.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountConsumer5.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,57 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+public class KafkaMyWordCountConsumer5 {\n+    static String topic = \"my-word5-by-year\";\n+\n+    public static void main(String[] args) throws InterruptedException {\n+//        test_producer();\n+//\n+        test_consumer();\n+//\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException {\n+\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"group.id\", \"MyGroup\" + System.currentTimeMillis());\n+        props.put(\"key.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(\"value.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false \");\n+        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n+\n+        consumer.subscribe(Collections.singletonList(topic));\n+\n+        Duration timeout = Duration.ofMillis(100);\n+        System.out.println(\"Consuming.......\");\n+        while (true) {\n+            ConsumerRecords<String, String> records = consumer.poll(timeout);\n+            for (ConsumerRecord<String, String> record : records) {\n+                System.out.printf(System.currentTimeMillis() + \" KafkaMyWordCountConsumer::::: topic = %s, partition = %d, offset = %d, \" +\n+                                \"k = %s, value = %s\\n\",\n+                        record.topic(), record.partition(), record.offset(),\n+                        record.key(), record.value());\n+                //int updatedCount = 1;\n+                //System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+            }\n+            Thread.sleep(100);\n+        }\n+\n+\n+\n+    }\n+\n+}"
  },
  {
    "sha": "c2599e1fc185c2fcacdbccf08862bfecc8b20797",
    "filename": "complete/src/test/java/com/example/springboot/KafkaMyWordCountProducer.java",
    "status": "added",
    "additions": 104,
    "deletions": 0,
    "changes": 104,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountProducer.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountProducer.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountProducer.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,104 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+\n+public class KafkaMyWordCountProducer {\n+    static String topic = \"test1\";\n+    static String topic2 = \"test2\";\n+\n+    public static void main(String[] args) throws InterruptedException, ExecutionException {\n+        test_producer();\n+//\n+//        test_consumer();\n+\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException {\n+\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"group.id\", \"MyGroup2\");\n+        props.put(\"key.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(\"value.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+\n+        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n+\n+        consumer.subscribe(Collections.singletonList(topic));\n+\n+        Duration timeout = Duration.ofMillis(100);\n+        System.out.println(\"Consuming.......\");\n+        while (true) {\n+            ConsumerRecords<String, String> records = consumer.poll(timeout);\n+            for (ConsumerRecord<String, String> record : records) {\n+                System.out.printf(\"topic = %s, partition = %d, offset = %d, \" +\n+                                \"customer = %s, country = %s\\n\",\n+                        record.topic(), record.partition(), record.offset(),\n+                        record.key(), record.value());\n+                int updatedCount = 1;\n+                System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+            }\n+            Thread.sleep(100);\n+        }\n+\n+\n+\n+    }\n+\n+    private static void test_producer() throws ExecutionException, InterruptedException {\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"key.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"value.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+\n+        /*\n+apple 2021 1\n+apple 2022 2\n+apple 2023 3\n+orange 2020 1\n+\n+total sum\n+total average\n+total count by type\n+total count by year\n+\n+\n+        * */\n+\n+        KafkaProducer producer = new KafkaProducer<String, String>(props);\n+        long t1 = System.currentTimeMillis();\n+//        producer.send(new ProducerRecord<>(topic, null, \"apple 1999 1\")).get();\n+//        producer.send(new ProducerRecord<>(topic, null, \"orange 2000 1\")).get();\n+//        producer.send(new ProducerRecord<>(topic, null, \"apple 2000 1\")).get();\n+//        producer.send(new ProducerRecord<>(topic, null, \"banana 2001 5\")).get();\n+//        producer.send(new ProducerRecord<>(topic, null, \"apple 2002 1\")).get();\n+        for(int i=0; i<10; i++){\n+            producer.send(new ProducerRecord<>(topic2, null, \"bananaXEE \" + i)).get();\n+//            producer.send(new ProducerRecord<>(topic, null, \"orange 2024 1\")).get();\n+        }\n+//        producer.send(new ProducerRecord<>(topic, \"orange\", \"3000 3\")).get();\n+\n+        System.out.println(\"Sent out. \" + (System.currentTimeMillis() - t1));\n+\n+        // 200 million    one request:  1K header, ip, URL, 512bytes.  3K\n+        // 600GB//\n+        //per server we have about\n+\n+        //how many partitions do you need?\n+        //The output\n+\n+    }\n+}"
  },
  {
    "sha": "ac323d13184cf7d2be5de6eb32971278bd2a755c",
    "filename": "complete/src/test/java/com/example/springboot/KafkaMyWordCountStream.java",
    "status": "added",
    "additions": 67,
    "deletions": 0,
    "changes": 67,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountStream.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountStream.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountStream.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,67 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Reducer;\n+\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+\n+public class KafkaMyWordCountStream {\n+    static String topic = \"my-word2\";\n+\n+\n+    public static void main(String[] args) throws InterruptedException, ExecutionException {\n+\n+        Properties props = new Properties();\n+        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"testApp3\");\n+        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n+        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n+        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n+\n+        // setting offset reset to earliest so that we can re-run the demo code with the same pre-loaded data\n+        // Note: To re-run the demo, you need to use the offset reset tool:\n+        // https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Application+Reset+Tool\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+\n+\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        KStream<String, String> source = builder.stream(topic);\n+\n+        source.map((k, v) -> new KeyValue<>(v.split(\" \")[1], v.split(\" \")[2]))\n+                .groupByKey().reduce(new SumReducer()).toStream().mapValues(v -> v.toString() + \" total Num \")\n+                .to(\"my-word3-by-year\");\n+\n+\n+        KafkaStreams streams = new KafkaStreams(builder.build(), props);\n+\n+        // This is for reset to work. Don't use in production - it causes the app to re-load the state from Kafka on every start\n+//        streams.cleanUp();\n+\n+        System.out.println(\"Processing the stream....\");\n+        streams.start();\n+\n+        // usually the stream application would be running forever,\n+        // in this example we just let it run for some time and stop since the input data is finite.\n+        //Thread.sleep(5000L);\n+\n+        //streams.close();\n+\n+    }\n+\n+    public static class SumReducer implements Reducer<String>{\n+\n+        @Override\n+        public String apply(String value1, String value2) {\n+            return \"\" +(Integer.valueOf(value1) + Integer.valueOf(value2));\n+        }\n+    }\n+\n+\n+}"
  },
  {
    "sha": "2a75aa3db4c89c03973f5dbb27f58f4d3931af3f",
    "filename": "complete/src/test/java/com/example/springboot/KafkaMyWordCountStreamByName.java",
    "status": "added",
    "additions": 67,
    "deletions": 0,
    "changes": 67,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountStreamByName.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountStreamByName.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountStreamByName.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,67 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Reducer;\n+\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+\n+public class KafkaMyWordCountStreamByName {\n+    static String topic = \"my-word2\";\n+\n+\n+    public static void main(String[] args) throws InterruptedException, ExecutionException {\n+\n+        Properties props = new Properties();\n+        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"testApp\" + System.currentTimeMillis());\n+        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n+        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n+        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n+\n+        // setting offset reset to earliest so that we can re-run the demo code with the same pre-loaded data\n+        // Note: To re-run the demo, you need to use the offset reset tool:\n+        // https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Application+Reset+Tool\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+\n+\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        KStream<String, String> source = builder.stream(topic);\n+\n+        source.map((k, v) -> new KeyValue<>(v.split(\" \")[0], v.split(\" \")[2]))\n+                .groupByKey().reduce(new SumReducer()).toStream().mapValues(v -> v.toString() + \" total Num \")\n+                .to(\"my-word-by-name\");\n+\n+\n+        KafkaStreams streams = new KafkaStreams(builder.build(), props);\n+\n+        // This is for reset to work. Don't use in production - it causes the app to re-load the state from Kafka on every start\n+//        streams.cleanUp();\n+\n+        System.out.println(\"Processing the stream....\");\n+        streams.start();\n+\n+        // usually the stream application would be running forever,\n+        // in this example we just let it run for some time and stop since the input data is finite.\n+        //Thread.sleep(5000L);\n+\n+        //streams.close();\n+\n+    }\n+\n+    public static class SumReducer implements Reducer<String>{\n+\n+        @Override\n+        public String apply(String value1, String value2) {\n+            return \"\" +(Integer.valueOf(value1) + Integer.valueOf(value2));\n+        }\n+    }\n+\n+\n+}"
  },
  {
    "sha": "df0a72409a407b663389ba467feb3e9f0481f4cd",
    "filename": "complete/src/test/java/com/example/springboot/KafkaMyWordCountStreamByWindow.java",
    "status": "added",
    "additions": 71,
    "deletions": 0,
    "changes": 71,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountStreamByWindow.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountStreamByWindow.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaMyWordCountStreamByWindow.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,71 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.kstream.*;\n+\n+import java.time.Duration;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+\n+public class KafkaMyWordCountStreamByWindow {\n+    static String topic = \"my-word2\";\n+\n+\n+    public static void main(String[] args) throws InterruptedException, ExecutionException {\n+\n+        Properties props = new Properties();\n+        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"testApp\" + System.currentTimeMillis());\n+        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n+        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n+        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n+\n+        // setting offset reset to earliest so that we can re-run the demo code with the same pre-loaded data\n+        // Note: To re-run the demo, you need to use the offset reset tool:\n+        // https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Application+Reset+Tool\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+\n+\n+        StreamsBuilder builder = new StreamsBuilder();\n+\n+        KStream<String, String> source = builder.stream(topic);\n+\n+        source.map((k, v) -> new KeyValue<>(v.split(\" \")[1], v.split(\" \")[2]))\n+                .groupByKey()\n+                .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofSeconds(30L)))\n+                .reduce(new SumReducer())\n+                .toStream()\n+                .mapValues(v -> v.toString() + \" total Num \")\n+//                .to(\"my-word5-by-year\");\n+                .to(\"my-word4-by-year\", Produced.keySerde(WindowedSerdes.timeWindowedSerdeFrom(String.class, 30000)));\n+\n+        KafkaStreams streams = new KafkaStreams(builder.build(), props);\n+\n+        // This is for reset to work. Don't use in production - it causes the app to re-load the state from Kafka on every start\n+//        streams.cleanUp();\n+\n+        System.out.println(\"Processing the stream....\");\n+        streams.start();\n+\n+        // usually the stream application would be running forever,\n+        // in this example we just let it run for some time and stop since the input data is finite.\n+        //Thread.sleep(5000L);\n+\n+        //streams.close();\n+\n+    }\n+\n+    public static class SumReducer implements Reducer<String>{\n+\n+        @Override\n+        public String apply(String value1, String value2) {\n+            return \"\" +(Integer.valueOf(value1) + Integer.valueOf(value2));\n+        }\n+    }\n+\n+\n+}"
  },
  {
    "sha": "5980b29730104466b168517f3060bad390cf034c",
    "filename": "complete/src/test/java/com/example/springboot/KafkaStockAskPriceProducer.java",
    "status": "added",
    "additions": 50,
    "deletions": 0,
    "changes": 50,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStockAskPriceProducer.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStockAskPriceProducer.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStockAskPriceProducer.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,50 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+\n+public class KafkaStockAskPriceProducer {\n+    static String topic = \"my-stock\";\n+\n+    public static void main(String[] args) throws InterruptedException, ExecutionException {\n+        test_producer();\n+    }\n+\n+    private static void test_producer() throws ExecutionException, InterruptedException {\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"key.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"value.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        /*\n+apple 2021 1\n+apple 2022 2\n+apple 2023 3\n+orange 2020 1\n+\n+total sum\n+total average\n+total count by type\n+total count by year\n+\n+\n+        * */\n+\n+        KafkaProducer producer = new KafkaProducer<String, String>(props);\n+\n+//        producer.send(new ProducerRecord<>(topic, null, \"apple 1999 1\")).get();\n+//        producer.send(new ProducerRecord<>(topic, null, \"orange 2000 1\")).get();\n+//        producer.send(new ProducerRecord<>(topic, null, \"apple 2000 1\")).get();\n+//        producer.send(new ProducerRecord<>(topic, null, \"banana 2001 5\")).get();\n+//        producer.send(new ProducerRecord<>(topic, null, \"apple 2002 1\")).get();\n+        producer.send(new ProducerRecord<>(topic, null, \"apple 2023 1\")).get();\n+        producer.send(new ProducerRecord<>(topic, null, \"orange 2024 1\")).get();\n+//        producer.send(new ProducerRecord<>(topic, \"orange\", \"3000 3\")).get();\n+\n+        System.out.println(\"Sent out. \");\n+    }\n+}"
  },
  {
    "sha": "b4e2e757d077ba3e2f9c2a2e24b4bbf75449b98d",
    "filename": "complete/src/test/java/com/example/springboot/KafkaStringConsumeManualOffset.java",
    "status": "added",
    "additions": 148,
    "deletions": 0,
    "changes": 148,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumeManualOffset.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumeManualOffset.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumeManualOffset.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,148 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.*;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.time.ZoneId;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class KafkaStringConsumeManualOffset {\n+    static String topic = \"my\";\n+\n+    public static void main(String[] args) throws InterruptedException {\n+//        test_producer();\n+//\n+        test_consumer();\n+\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException {\n+\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        String myGroup5 = \"MyGroup7x\";\n+        props.put(\"group.id\", myGroup5);\n+        props.put(\"key.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(\"value.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        //props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false \");\n+\n+        KafkaConsumer<String, String> consumer =  new KafkaConsumer<String, String>(props);\n+\n+        Duration timeout = Duration.ofMillis(5000);\n+        System.out.println(\"Consuming....... consumer group:\" + myGroup5);\n+\n+        //commented this subscribe().\n+//        consumer.subscribe(Collections.singletonList(topic));\n+\n+//        Set<TopicPartition> assignment1 = consumer.assignment();\n+        List<TopicPartition> manualAssignment = new ArrayList();\n+        manualAssignment.add(new TopicPartition(topic, 0));\n+        manualAssignment.add(new TopicPartition(topic, 1));\n+        consumer.assign(manualAssignment);\n+        consumer.seekToEnd(manualAssignment);\n+\n+        Long oneHourEarlier = Instant.now().atZone(ZoneId.systemDefault()).minusMinutes(1).toEpochSecond();\n+        Set<TopicPartition> assignment = consumer.assignment();\n+        Map<TopicPartition, Long> partitionTimestampMap = assignment.stream().collect(Collectors.toMap(tp -> tp, tp -> oneHourEarlier));\n+        Map<TopicPartition, OffsetAndTimestamp> offsetMap = consumer.offsetsForTimes(partitionTimestampMap);\n+\n+        for(Map.Entry<TopicPartition,OffsetAndTimestamp> entry: offsetMap.entrySet()) {\n+            if(entry.getValue()!=null){\n+                consumer.seek(entry.getKey(), entry.getValue().offset());\n+            }\n+        }\n+\n+\n+        while (true) {\n+            System.out.println(\"polling...\" + System.currentTimeMillis());\n+\n+            ConsumerRecords<String, String> records = consumer.poll(timeout);\n+            for (ConsumerRecord<String, String> record : records) {\n+                System.out.printf(\"topic = %s, partition = %d, offset = %d, \" +\n+                                \"KEY = %s, VALUE = %s\\n\",\n+                        record.topic(), record.partition(), record.offset(),\n+                        record.key(), record.value());\n+                consumer.commitAsync();\n+//                int updatedCount = 1;\n+//                System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+            }\n+        }\n+\n+//        List<PartitionInfo> partitionInfos = consumer.partitionsFor(topic);\n+//        ArrayList<TopicPartition> partitions = new ArrayList<>();\n+//        for(PartitionInfo info : partitionInfos){\n+//            partitions.add(new TopicPartition(info.topic(), info.partition()));\n+//        }\n+//\n+//        for(TopicPartition partition : assignment1){\n+//            consumer.seek(partition, 1);\n+//        }\n+//\n+//        Long oneHourEarlier = Instant.now().atZone(ZoneId.systemDefault()).minusHours(100).toEpochSecond();\n+//\n+//        Map<TopicPartition, Long> partitionTimestampMap = assignment1.stream().collect(Collectors.toMap(tp -> tp, tp -> oneHourEarlier));\n+//        Map<TopicPartition, OffsetAndTimestamp> offsetMap = consumer.offsetsForTimes(partitionTimestampMap);\n+//        System.out.println(\"size: \" + offsetMap.size());\n+//        for(Map.Entry<TopicPartition,OffsetAndTimestamp> entry: offsetMap.entrySet()) {\n+//            System.out.println(entry.hashCode() + \" \" + entry.getKey() + \" \" + entry.getValue() + \" \");\n+////            System.out.println(consumer.seek(entry.getKey(), entry.getValue().offset()));\n+//\n+//        }\n+//        Set<TopicPartition> assignment = assignment1;\n+//        for (TopicPartition p : assignment) {\n+//            System.out.println(p.hashCode() + \" \" + p.topic() + \" \" + p.partition() + \" \");\n+//        }\n+//        long oneHourEalier = System.currentTimeMillis() - 1000 * 60 * 60;\n+//        Map<TopicPartition, OffsetAndTimestamp> topicPartitionOffsetAndTimestampMap = consumer.offsetsForTimes(assignment.stream().collect(Collectors.toMap(tp -> tp, tp -> oneHourEalier)));\n+//        for (TopicPartition key : topicPartitionOffsetAndTimestampMap.keySet()) {\n+//            long offset = topicPartitionOffsetAndTimestampMap.get(key).offset();\n+//            System.out.println(\"topicPartitionOffsetAndTimestampMap offset:\" + offset);\n+//\n+//        }\n+        //ConsumerRecords<String, String> records = consumer.poll(timeout);\n+//\n+//            if(records.count() > 0){\n+//                System.out.println(\"Got \" + records.count() + \" records.\");\n+//            }\n+//            for (ConsumerRecord<String, String> record : records) {\n+//                System.out.printf(\"topic = %s, partition = %d, offset = %d, \" +\n+//                                \"KEY = %s, VALUE = %s\\n\",\n+//                        record.topic(), record.partition(), record.offset(),\n+//                        record.key(), record.value());\n+//                consumer.commitAsync();\n+////                int updatedCount = 1;\n+////                System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+//            }\n+//            consumer.commitSync();\n+\n+\n+    }\n+\n+    private static void test_producer() {\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n+\n+\n+        ProducerRecord<String, String> record = new ProducerRecord<>(topic, \"k2\", \"France222\");\n+        KafkaProducer producer = new KafkaProducer<String, String>(props);\n+        try {\n+            producer.send(record).get();\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+        }\n+        System.out.println(\"Sent out. \");\n+    }\n+}"
  },
  {
    "sha": "b82b42ef2aad11b0ab1e959cd4b3a710ce52d09c",
    "filename": "complete/src/test/java/com/example/springboot/KafkaStringConsumer1.java",
    "status": "added",
    "additions": 103,
    "deletions": 0,
    "changes": 103,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumer1.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumer1.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumer1.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,103 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.*;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+public class KafkaStringConsumer1 {\n+    static String topic = \"my\";\n+\n+    public static void main(String[] args) throws InterruptedException {\n+//        test_producer();\n+//\n+        test_consumer();\n+\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException {\n+\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        String myGroup5 = \"MyGroup5\";\n+        props.put(\"group.id\", myGroup5);\n+        props.put(\"key.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(\"value.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false \");\n+\n+        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n+\n+        consumer.subscribe(Collections.singletonList(topic), new ConsumerRebalanceListener() {\n+            @Override\n+            public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                System.out.println(\"onPartitionsRevoked--------... \");\n+                for(TopicPartition partition : partitions){\n+                    System.out.println(\"onPartitionsRevoked--------: \" + partition.hashCode() + \" \" + partition.hashCode());\n+                }\n+\n+            }\n+\n+            @Override\n+            public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                System.out.println(\"onPartitionsAssigned... \");\n+                for(TopicPartition partition : partitions){\n+                    System.out.println(\"onPartitionsAssigned: \" + partition.hashCode() + \" \" + partition.hashCode());\n+                }\n+\n+            }\n+        });\n+\n+        Duration timeout = Duration.ofMillis(5000);\n+        System.out.println(\"Consuming....... consumer group:\" + myGroup5);\n+        int i = 0;\n+        while (i++ < 1000) {\n+            System.out.println(\"polling...\" + System.currentTimeMillis());\n+\n+            ConsumerRecords<String, String> records = consumer.poll(timeout);\n+            if(records.count() > 0){\n+                System.out.println(\"Got \" + records.count() + \" records.\");\n+            }\n+            for (ConsumerRecord<String, String> record : records) {\n+                System.out.printf(\"topic = %s, partition = %d, offset = %d, \" +\n+                                \"KEY = %s, VALUE = %s\\n\",\n+                        record.topic(), record.partition(), record.offset(),\n+                        record.key(), record.value());\n+                consumer.commitAsync();\n+//                int updatedCount = 1;\n+//                System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+            }\n+            consumer.commitSync();\n+        }\n+\n+\n+\n+    }\n+\n+    private static void test_producer() {\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"key.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"value.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+\n+\n+        ProducerRecord<String, String> record = new ProducerRecord<>(topic, \"k2\", \"France222\");\n+        KafkaProducer producer = new KafkaProducer<String, String>(props);\n+        try {\n+            producer.send(record).get();\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+        }\n+        System.out.println(\"Sent out. \");\n+    }\n+}"
  },
  {
    "sha": "bf2d8e44d806237055fd315cc786a923ee0185f7",
    "filename": "complete/src/test/java/com/example/springboot/KafkaStringConsumer11.java",
    "status": "added",
    "additions": 96,
    "deletions": 0,
    "changes": 96,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumer11.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumer11.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumer11.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,96 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.*;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+public class KafkaStringConsumer11 {\n+    static String topic = \"my\";\n+\n+    public static void main(String[] args) throws InterruptedException {\n+//        test_producer();\n+//\n+        test_consumer();\n+\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException {\n+\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        String myGroup5 = \"MyGroup7x\";\n+        props.put(\"group.id\", myGroup5);\n+        props.put(\"key.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(\"value.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false \");\n+\n+        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n+\n+        consumer.subscribe(Collections.singletonList(topic), new ConsumerRebalanceListener() {\n+            @Override\n+            public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                System.out.println(\"Going to end\");\n+            }\n+\n+            @Override\n+            public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                System.out.println(\"New Client2\");\n+\n+            }\n+        });\n+\n+        Duration timeout = Duration.ofMillis(5000);\n+        System.out.println(\"Consuming....... consumer group:\" + myGroup5);\n+        int i = 0;\n+        while (i++ < 1000) {\n+            System.out.println(\"polling...\" + System.currentTimeMillis());\n+\n+            ConsumerRecords<String, String> records = consumer.poll(timeout);\n+            if(records.count() > 0){\n+                System.out.println(\"Got \" + records.count() + \" records.\");\n+            }\n+            for (ConsumerRecord<String, String> record : records) {\n+                System.out.printf(\"topic = %s, partition = %d, offset = %d, \" +\n+                                \"KEY = %s, VALUE = %s\\n\",\n+                        record.topic(), record.partition(), record.offset(),\n+                        record.key(), record.value());\n+                consumer.commitAsync();\n+//                int updatedCount = 1;\n+//                System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+            }\n+            consumer.commitSync();\n+        }\n+\n+\n+\n+    }\n+\n+    private static void test_producer() {\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"key.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"value.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+\n+\n+        ProducerRecord<String, String> record = new ProducerRecord<>(\"kafka-config-topic\", \"k2\", \"France222\");\n+        KafkaProducer producer = new KafkaProducer<String, String>(props);\n+        try {\n+            producer.send(record).get();\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+        }\n+        System.out.println(\"Sent out. \");\n+    }\n+}"
  },
  {
    "sha": "8a4605c3e7b3c4156cddba92678a48c9a8d6fc2f",
    "filename": "complete/src/test/java/com/example/springboot/KafkaStringConsumerExactlyOneTransaction.java",
    "status": "added",
    "additions": 90,
    "deletions": 0,
    "changes": 90,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumerExactlyOneTransaction.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumerExactlyOneTransaction.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaStringConsumerExactlyOneTransaction.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,90 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.time.Duration;\n+import java.util.*;\n+\n+public class KafkaStringConsumerExactlyOneTransaction {\n+    static String topic = \"my\";\n+\n+    public static void main(String[] args) throws InterruptedException {\n+//        test_producer();\n+//\n+        test_consumer();\n+\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException {\n+\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        String myGroup5 = \"MyGroup6\";\n+        props.put(\"group.id\", myGroup5);\n+        props.put(\"key.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(\"value.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false \");\n+\n+        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n+\n+        consumer.subscribe(Collections.singletonList(topic));\n+\n+        List<TopicPartition> partitionList = new ArrayList();\n+\n+        Map<String, List<PartitionInfo>> stringListMap = consumer.listTopics();\n+        for(String s : stringListMap.keySet()){\n+            List<PartitionInfo> partitionInfos = stringListMap.get(s);\n+            for(PartitionInfo info : partitionInfos){\n+                //consumer.beginningOffsets()\n+            }\n+        }\n+\n+        Duration timeout = Duration.ofMillis(5000);\n+        System.out.println(\"Consuming....... consumer group:\" + myGroup5);\n+        while (true) {\n+            System.out.println(\"polling...\" + System.currentTimeMillis());\n+            ConsumerRecords<String, String> records = consumer.poll(timeout);\n+            for (ConsumerRecord<String, String> record : records) {\n+                System.out.printf(\"topic = %s, partition = %d, offset = %d, \" +\n+                                \"KEY = %s, VALUE = %s\\n\",\n+                        record.topic(), record.partition(), record.offset(),\n+                        record.key(), record.value());\n+//                int updatedCount = 1;\n+//                System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+            }\n+        }\n+\n+\n+\n+    }\n+\n+    private static void test_producer() {\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"key.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"value.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+\n+\n+        ProducerRecord<String, String> record = new ProducerRecord<>(topic, \"k2\", \"France222\");\n+        KafkaProducer producer = new KafkaProducer<String, String>(props);\n+        try {\n+            producer.send(record).get();\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+        }\n+        System.out.println(\"Sent out. \");\n+    }\n+}"
  },
  {
    "sha": "e2e2609216a49607ab0fe17826823b3c81fcb614",
    "filename": "complete/src/test/java/com/example/springboot/KafkaWordCountConsumer.java",
    "status": "added",
    "additions": 77,
    "deletions": 0,
    "changes": 77,
    "blob_url": "https://github.com/random1223/gs-spring-boot/blob/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaWordCountConsumer.java",
    "raw_url": "https://github.com/random1223/gs-spring-boot/raw/d23b10ffe09bdffd33ed5e8263560e1601e7a053/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaWordCountConsumer.java",
    "contents_url": "https://api.github.com/repos/random1223/gs-spring-boot/contents/complete%2Fsrc%2Ftest%2Fjava%2Fcom%2Fexample%2Fspringboot%2FKafkaWordCountConsumer.java?ref=d23b10ffe09bdffd33ed5e8263560e1601e7a053",
    "patch": "@@ -0,0 +1,77 @@\n+package com.example.springboot;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+public class KafkaWordCountConsumer {\n+    static String topic = \"my-output\";\n+\n+    public static void main(String[] args) throws InterruptedException {\n+//        test_producer();\n+        test_consumer();\n+\n+\n+    }\n+\n+    private static void test_consumer() throws InterruptedException {\n+\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"group.id\", \"MyGroup2\");\n+        props.put(\"key.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(\"value.deserializer\",\n+                \"org.apache.kafka.common.serialization.StringDeserializer\");\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false \");\n+\n+        KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);\n+\n+        consumer.subscribe(Collections.singletonList(topic));\n+\n+        Duration timeout = Duration.ofMillis(5000);\n+        while (true) {\n+            System.out.println(\"Consuming.......\" + System.currentTimeMillis());\n+            ConsumerRecords<String, String> records = consumer.poll(timeout);\n+            for (ConsumerRecord<String, String> record : records) {\n+                System.out.printf(\"KafkaWordCountConsumer: topic = %s, partition = %d, offset = %d, \" +\n+                                \"key = %s, value = %s\\n\",\n+                        record.topic(), record.partition(), record.offset(),\n+                        record.key(), record.value());\n+//                int updatedCount = 1;\n+//                System.out.println(\"Got: key: \" + record.key() + \" value: \" + record.value());\n+            }\n+//            Thread.sleep(100);\n+        }\n+\n+\n+\n+    }\n+\n+    private static void test_producer() {\n+        Properties props = new Properties();\n+        props.put(\"bootstrap.servers\", \"localhost:9092\");\n+        props.put(\"key.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"value.serializer\", \"io.confluent.kafka.serializers.KafkaAvroSerializer\");\n+        props.put(\"key.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+        props.put(\"value.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\");\n+\n+\n+        ProducerRecord<String, String> record = new ProducerRecord<>(topic, \"k1\", \"France222\");\n+        KafkaProducer producer = new KafkaProducer<String, String>(props);\n+        try {\n+            producer.send(record).get();\n+        } catch (Exception e) {\n+            e.printStackTrace();\n+        }\n+        System.out.println(\"Sent out. \");\n+    }\n+}"
  }
]